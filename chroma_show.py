import os
import gradio as gr
import chromadb
import numpy as np
import pandas as pd
from chromadb.config import Settings
import json
from collections import defaultdict
import concurrent.futures
import threading
from typing import Dict, List
import time
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼Œåˆ›å»ºæ—¥å¿—ç›®å½•å’Œæ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°"""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    log_filename = os.path.join(log_dir, f"chroma_client_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s [%(levelname)s] %(name)s:%(funcName)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

class ChromaClient:
    def __init__(self, path):
        """åˆå§‹åŒ–ChromaClientï¼Œè®¾ç½®æ•°æ®åº“è·¯å¾„å’Œæ—¥å¿—"""
        self.logger = setup_logging()
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–ChromaClient")
        
        settings_dict = {"allow_reset": True, "anonymized_telemetry": False}
        try:
            self.client = chromadb.PersistentClient(path=path, settings=Settings(**settings_dict))
            self.filename_to_collections = defaultdict(list)
            self.is_mapping_initialized = False
            self._lock = threading.Lock()
            self.progress = {"current": 0, "total": 0, "status": ""}
            self.last_update_time = 0  # ç”¨äºç¼“å­˜åˆ·æ–°æ—¶é—´æˆ³
            self.logger.info(f"ChromaClientåˆå§‹åŒ–æˆåŠŸï¼Œæ•°æ®åº“è·¯å¾„: {path}")
        except Exception as e:
            self.logger.error(f"ChromaClientåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def list_collections(self):
        """è·å–æ‰€æœ‰Collectionåç§°"""
        self.logger.info("è·å–Collectionåˆ—è¡¨")
        try:
            collections = self.client.list_collections()
            result = [col.name for col in collections] if collections else ["æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Collection"]
            self.logger.debug(f"æ‰¾åˆ° {len(result)} ä¸ªCollection")
            return result
        except Exception as e:
            self.logger.error(f"è·å–Collectionåˆ—è¡¨å¤±è´¥: {str(e)}")
            return ["è·å–Collectionåˆ—è¡¨å‡ºé”™"]

    def process_collection_batch(self, collections):
        """æ‰¹é‡å¤„ç†Collectionï¼Œè·å–æ–‡ä»¶åå’ŒCollectionæ˜ å°„"""
        self.logger.debug(f"å¤„ç† {len(collections)} ä¸ªCollectionæ‰¹æ¬¡")
        results = []
        for col in collections:
            try:
                result = col.get(limit=1, include=["metadatas"])  # åªè·å–å…ƒæ•°æ®ï¼Œå‡å°‘æ•°æ®é‡
                if result['metadatas'] and result['metadatas'][0]:
                    filename = result['metadatas'][0].get('name', '')
                    if filename:
                        results.append((filename, col.name))
                        self.logger.debug(f"å¤„ç†Collection {col.name}ï¼Œå…³è”æ–‡ä»¶å: {filename}")
            except Exception as e:
                self.logger.error(f"å¤„ç†Collection {col.name} å¤±è´¥: {str(e)}")
        return results

    def lazy_update_filename_mapping(self, force_refresh=False) -> str:
        """å»¶è¿Ÿæ›´æ–°æ–‡ä»¶ååˆ°Collectionçš„æ˜ å°„ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼Œå¹¶æ”¯æŒç¼“å­˜"""
        current_time = time.time()
        if self.is_mapping_initialized and not force_refresh and (current_time - self.last_update_time < 300):  # ç¼“å­˜5åˆ†é’Ÿ
            self.logger.info("æ–‡ä»¶åæ˜ å°„å·²åˆå§‹åŒ–ä¸”æœªè¿‡æœŸï¼Œè·³è¿‡æ›´æ–°")
            return "æ–‡ä»¶åˆ—è¡¨å·²åŠ è½½ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"

        self.logger.info("å¼€å§‹æ›´æ–°æ–‡ä»¶åæ˜ å°„")
        try:
            collections = self.client.list_collections()
            total = len(collections)
            if total == 0:
                self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•Collection")
                return "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Collection"

            self.progress = {"current": 0, "total": total, "status": "å¼€å§‹åŠ è½½..."}
            start_time = time.time()

            # åˆ†æ‰¹å¤„ç†Collectionï¼Œè°ƒæ•´æ‰¹æ¬¡å¤§å°
            batch_size = 100  # å¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œå‡å°‘çº¿ç¨‹å¼€é”€
            collection_batches = [collections[i:i + batch_size] for i in range(0, total, batch_size)]

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ŒåŠ¨æ€è°ƒæ•´workeræ•°é‡
            max_workers = min(20, max(1, os.cpu_count() or 1))  # æ ¹æ®CPUæ ¸å¿ƒæ•°åŠ¨æ€è°ƒæ•´
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_batch = {executor.submit(self.process_collection_batch, batch): i 
                                  for i, batch in enumerate(collection_batches)}

                self.filename_to_collections.clear()

                for future in concurrent.futures.as_completed(future_to_batch):
                    batch_results = future.result()
                    batch_index = future_to_batch[future]
                    
                    with self._lock:
                        for filename, col_name in batch_results:
                            self.filename_to_collections[filename].append(col_name)
                        processed = min((batch_index + 1) * batch_size, total)
                        self.progress["current"] = processed
                        percentage = (processed / total) * 100
                        self.progress["status"] = f"å·²å¤„ç†: {processed}/{total} ({percentage:.1f}%)"

            end_time = time.time()
            duration = end_time - start_time
            total_files = len(self.filename_to_collections)
            total_mappings = sum(len(cols) for cols in self.filename_to_collections.values())
            
            self.is_mapping_initialized = True
            self.last_update_time = current_time
            self.logger.info(f"æ–‡ä»¶åæ˜ å°„æ›´æ–°å®Œæˆï¼Œå¤„ç† {total} ä¸ªCollectionï¼Œæ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_mappings} ä¸ªæ˜ å°„ï¼Œç”¨æ—¶ {duration:.2f} ç§’")
            return (f"åŠ è½½å®Œæˆ! å¤„ç†äº† {total} ä¸ªcollections, "
                    f"æ‰¾åˆ° {total_files} ä¸ªå”¯ä¸€æ–‡ä»¶å, "
                    f"å…± {total_mappings} ä¸ªæ˜ å°„, "
                    f"ç”¨æ—¶ {duration:.2f} ç§’")
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ–‡ä»¶åæ˜ å°„å¤±è´¥: {str(e)}")
            return f"æ›´æ–°æ–‡ä»¶åæ˜ å°„å‡ºé”™: {str(e)}"

    def get_loading_progress(self) -> str:
        """è·å–å½“å‰åŠ è½½è¿›åº¦"""
        progress = self.progress["status"]
        self.logger.debug(f"æŸ¥è¯¢åŠ è½½è¿›åº¦: {progress}")
        return progress

    def list_filenames(self):
        """è·å–æ‰€æœ‰åŸå§‹æ–‡ä»¶å"""
        filenames = sorted(list(self.filename_to_collections.keys()))
        self.logger.info(f"è·å–æ–‡ä»¶ååˆ—è¡¨ï¼Œå…± {len(filenames)} ä¸ªæ–‡ä»¶")
        return filenames

    def get_collections_by_filename(self, filename):
        """é€šè¿‡æ–‡ä»¶åè·å–ç›¸å…³çš„Collection"""
        collections = self.filename_to_collections.get(filename, [])
        self.logger.debug(f"æŸ¥è¯¢æ–‡ä»¶å {filename} çš„Collectionï¼Œæ‰¾åˆ° {len(collections)} ä¸ª")
        return collections

    def get_collection_content(self, collection_name: str):
        """è·å–Collectionå†…å®¹å¹¶è®°å½•æ“ä½œï¼Œå±•ç¤ºæ‰€æœ‰åˆ†æ®µ"""
        if not collection_name or collection_name in ["æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Collection", "è·å–Collectionåˆ—è¡¨å‡ºé”™"]:
            self.logger.warning("æ— æ•ˆçš„Collectionåç§°")
            return "è¯·é€‰æ‹©æœ‰æ•ˆçš„Collection"
        
        self.logger.info(f"è·å–Collection {collection_name} çš„å†…å®¹")
        try:
            collection = self.client.get_collection(name=collection_name)
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                metadata_future = executor.submit(lambda: collection.get(limit=1))
                count_future = executor.submit(collection.count)
                result = metadata_future.result()
                count = count_future.result()

            if not result['ids']:
                self.logger.warning(f"Collection {collection_name} ä¸ºç©º")
                return "Collectionä¸ºç©º"

            file_info = {}
            embedding_config = None
            for metadata in result['metadatas']:
                if metadata.get('file_id') and metadata.get('name'):
                    file_info['file_id'] = metadata.get('file_id')
                    file_info['filename'] = metadata.get('name')
                    file_info['hash'] = metadata.get('hash')
                    file_info['source'] = metadata.get('source')
                    if metadata.get('embedding_config'):
                        try:
                            embedding_config = json.loads(metadata.get('embedding_config'))
                        except:
                            pass
                    break

            info = "æ–‡ä»¶ä¿¡æ¯:\n"
            info += f"æ–‡ä»¶å: {file_info.get('filename', 'æœªçŸ¥')}\n"
            info += f"æ–‡ä»¶ID: {file_info.get('file_id', 'æœªçŸ¥')}\n"
            info += f"æ–‡ä»¶å“ˆå¸Œ: {file_info.get('hash', 'æœªçŸ¥')}\n"
            info += f"æ–‡ä»¶è·¯å¾„: {file_info.get('source', 'æœªçŸ¥')}\n"
            
            if embedding_config:
                info += f"\nåµŒå…¥æ¨¡å‹é…ç½®:\n"
                info += f"å¼•æ“: {embedding_config.get('engine', 'æœªçŸ¥')}\n"
                info += f"æ¨¡å‹: {embedding_config.get('model', 'æœªçŸ¥')}\n"
            
            info += f"\nCollectionç»Ÿè®¡:\n"
            info += f"æ–‡æ¡£æ€»æ•°: {count}\n"
            if result['embeddings']:
                info += f"å‘é‡ç»´åº¦: {len(result['embeddings'][0])}\n"

            # åˆ†æ‰¹è·å–æ‰€æœ‰åˆ†æ®µ
            batch_size = 500
            num_batches = (count + batch_size - 1) // batch_size
            all_segments = []

            self.logger.info(f"å¼€å§‹è·å– {count} ä¸ªåˆ†æ®µï¼Œåˆ†ä¸º {num_batches} æ‰¹")
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                futures = [
                    executor.submit(
                        lambda offset: collection.get(
                            limit=batch_size,
                            offset=offset,
                            include=["documents", "metadatas"]  # ä¿®å¤ï¼šç§»é™¤ 'ids'
                        ),
                        i * batch_size
                    )
                    for i in range(num_batches)
                ]

                for future in concurrent.futures.as_completed(futures):
                    batch_data = future.result()
                    for id_, doc, metadata in zip(
                        batch_data['ids'],  # ids è‡ªåŠ¨åŒ…å«åœ¨è¿”å›ç»“æœä¸­
                        batch_data['documents'],
                        batch_data['metadatas']
                    ):
                        all_segments.append((id_, doc, metadata))

            # æŒ‰åˆ†æ®µç´¢å¼•æ’åºï¼ˆå¯é€‰ï¼‰
            all_segments.sort(key=lambda x: x[2].get('start_index', 0))

            # æ ¼å¼åŒ–æ‰€æœ‰åˆ†æ®µ
            info += "\næ–‡æ¡£åˆ†æ®µï¼ˆå…¨éƒ¨ï¼‰:\n"
            for i, (id_, doc, metadata) in enumerate(all_segments):
                info += f"\nåˆ†æ®µ #{i+1}:\n"
                info += f"åˆ†æ®µID: {id_}\n"
                info += f"èµ·å§‹ç´¢å¼•: {metadata.get('start_index', 'æœªçŸ¥')}\n"
                info += f"å†…å®¹: {doc[:200]}...\n"
                info += "-" * 50 + "\n"
            
            self.logger.debug(f"æˆåŠŸè·å–Collection {collection_name} çš„å…¨éƒ¨ {len(all_segments)} ä¸ªåˆ†æ®µ")
            return info
        except Exception as e:
            self.logger.error(f"è·å–Collection {collection_name} å†…å®¹å¤±è´¥: {str(e)}")
            return f"è·å–Collectionå†…å®¹å‡ºé”™: {str(e)}"

    def get_raw_file_content(self, collection_name: str):
        """è·å–å¹¶é‡å»ºå®Œæ•´æ–‡ä»¶å†…å®¹"""
        if not collection_name or collection_name in ["æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Collection", "è·å–Collectionåˆ—è¡¨å‡ºé”™"]:
            self.logger.warning("æ— æ•ˆçš„Collectionåç§°")
            return "è¯·é€‰æ‹©æœ‰æ•ˆçš„Collection"
        
        self.logger.info(f"è·å–Collection {collection_name} çš„åŸå§‹æ–‡ä»¶å†…å®¹")
        try:
            collection = self.client.get_collection(name=collection_name)
            count = collection.count()
            if count == 0:
                self.logger.warning(f"Collection {collection_name} ä¸ºç©º")
                return "Collectionä¸ºç©º"

            batch_size = 500
            num_batches = (count + batch_size - 1) // batch_size
            all_segments = []

            with concurrent.futures.ThreadPoolExecutor(max_workers=500) as executor:
                futures = [executor.submit(lambda o: collection.get(limit=batch_size, offset=o), i * batch_size) 
                           for i in range(num_batches)]
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    for i, doc in enumerate(result['documents']):
                        start_index = result['metadatas'][i].get('start_index', 0)
                        all_segments.append((start_index, doc))

            if not all_segments:
                self.logger.warning(f"Collection {collection_name} æ— å†…å®¹")
                return "Collectionä¸ºç©º"

            all_segments.sort(key=lambda x: x[0])
            content = "å®Œæ•´æ–‡ä»¶å†…å®¹:\n" + "="*50 + "\n\n" + "\n".join(seg[1] for seg in all_segments) + "\n\n" + "="*50
            self.logger.debug(f"æˆåŠŸé‡å»ºCollection {collection_name} çš„æ–‡ä»¶å†…å®¹")
            return content
        except Exception as e:
            self.logger.error(f"è·å–åŸå§‹æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
            return f"è·å–åŸå§‹æ–‡ä»¶å†…å®¹å‡ºé”™: {str(e)}"

    def delete_collections_by_filename(self, filename) -> str:
        """åˆ é™¤ä¸æ–‡ä»¶åç›¸å…³çš„æ‰€æœ‰Collection"""
        if not filename:
            self.logger.warning("æœªé€‰æ‹©æ–‡ä»¶å")
            return "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
        
        collections = self.filename_to_collections.get(filename, [])
        if not collections:
            self.logger.warning(f"æ–‡ä»¶å {filename} æ— å…³è”Collection")
            return "æœªæ‰¾åˆ°ä¸æ­¤æ–‡ä»¶åç›¸å…³çš„Collection"
        
        self.logger.info(f"å¼€å§‹åˆ é™¤æ–‡ä»¶å {filename} çš„ {len(collections)} ä¸ªCollection")
        try:
            deleted_count = 0
            for col_name in collections:
                try:
                    self.client.delete_collection(col_name)
                    deleted_count += 1
                    self.logger.debug(f"æˆåŠŸåˆ é™¤Collection {col_name}")
                except Exception as e:
                    self.logger.error(f"åˆ é™¤Collection {col_name} å¤±è´¥: {str(e)}")
            
            if deleted_count > 0:
                del self.filename_to_collections[filename]
                self.logger.info(f"å·²ä»æ˜ å°„ä¸­ç§»é™¤æ–‡ä»¶å {filename}")
            
            self.logger.info(f"åˆ é™¤å®Œæˆï¼ŒæˆåŠŸåˆ é™¤ {deleted_count}/{len(collections)} ä¸ªCollection")
            return f"æˆåŠŸåˆ é™¤äº† {deleted_count}/{len(collections)} ä¸ªcollections"
        except Exception as e:
            self.logger.error(f"åˆ é™¤Collectionå¤±è´¥: {str(e)}")
            return f"åˆ é™¤å‡ºé”™: {str(e)}"

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    client = ChromaClient("path/to/data/vector_db")  # OpenWebuiçš„å‘é‡åº“è·¯å¾„
    client.logger.info("åˆ›å»ºGradioç•Œé¢")
    
    with gr.Blocks(title="ChromaDB æ–‡ä»¶æŸ¥çœ‹å™¨") as app:
        gr.Markdown("## ğŸ“Š ChromaDB æ–‡ä»¶æŸ¥çœ‹å™¨")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### æŒ‰Collection IDæŸ¥çœ‹")
                collection_dropdown = gr.Dropdown(choices=client.list_collections(), label="é€‰æ‹©Collection", interactive=True)
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°Collectionåˆ—è¡¨")
            
            with gr.Column(scale=1):
                gr.Markdown("### æŒ‰æ–‡ä»¶åæŸ¥çœ‹")
                filename_dropdown = gr.Dropdown(choices=[], label="é€‰æ‹©æ–‡ä»¶å", interactive=True)
                with gr.Row():
                    refresh_files_btn = gr.Button("ğŸ”„ åŠ è½½/åˆ·æ–°æ–‡ä»¶åˆ—è¡¨")
                    delete_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­æ–‡ä»¶", variant="stop")

        with gr.Row(visible=False) as confirm_dialog:
            confirm_text = gr.Textbox(label="ç¡®è®¤ä¿¡æ¯", interactive=False)
            with gr.Row():
                confirm_btn = gr.Button("ç¡®è®¤åˆ é™¤", variant="stop")
                cancel_btn = gr.Button("å–æ¶ˆ")

        progress_display = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", interactive=False, value="å‡†å¤‡å°±ç»ª")
        
        with gr.Tabs():
            with gr.TabItem("æ–‡ä»¶ä¿¡æ¯"):
                info_display = gr.Textbox(label="æ–‡ä»¶è¯¦ç»†ä¿¡æ¯", interactive=False, lines=25)
            with gr.TabItem("åŸå§‹æ–‡ä»¶å†…å®¹"):
                content_display = gr.Textbox(label="å®Œæ•´æ–‡ä»¶å†…å®¹", interactive=False, lines=30)

        def refresh_filenames_with_progress():
            """å¼‚æ­¥åˆ·æ–°æ–‡ä»¶ååˆ—è¡¨å¹¶è®°å½•æ“ä½œ"""
            client.logger.info("åˆ·æ–°æ–‡ä»¶ååˆ—è¡¨")
            try:
                if not client.is_mapping_initialized:
                    status = client.lazy_update_filename_mapping()
                filenames = client.list_filenames()
                client.logger.debug(f"æ–‡ä»¶ååˆ—è¡¨åˆ·æ–°å®Œæˆï¼Œæ‰¾åˆ° {len(filenames)} ä¸ªæ–‡ä»¶")
                return [gr.update(choices=filenames, value=filenames[0] if filenames else None), "æ–‡ä»¶åˆ—è¡¨å·²æ›´æ–°"]
            except Exception as e:
                client.logger.error(f"åˆ·æ–°æ–‡ä»¶ååˆ—è¡¨å¤±è´¥: {str(e)}")
                return [gr.update(choices=[]), f"åŠ è½½æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}"]

        def on_filename_select(filename):
            if not filename:
                client.logger.warning("æœªé€‰æ‹©æ–‡ä»¶å")
                return [gr.update(), "è¯·é€‰æ‹©æ–‡ä»¶å", "è¯·é€‰æ‹©æ–‡ä»¶å"]
            collections = client.get_collections_by_filename(filename)
            if not collections:
                client.logger.warning(f"æ–‡ä»¶å {filename} æ— å…³è”Collection")
                return [gr.update(), "æœªæ‰¾åˆ°ä¸æ­¤æ–‡ä»¶åç›¸å…³çš„Collection", "æœªæ‰¾åˆ°ä¸æ­¤æ–‡ä»¶åç›¸å…³çš„Collection"]
            collection_name = collections[0]
            content = client.get_collection_content(collection_name)
            raw_content = client.get_raw_file_content(collection_name)
            client.logger.debug(f"æ–‡ä»¶å {filename} é€‰æ‹©å®Œæˆï¼Œå…³è”Collection: {collection_name}")
            return [gr.update(value=collection_name), content, raw_content]

        def on_collection_select(collection_name):
            if not collection_name or collection_name in ["æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Collection", "è·å–Collectionåˆ—è¡¨å‡ºé”™"]:
                client.logger.warning("æ— æ•ˆçš„Collectioné€‰æ‹©")
                return ["è¯·é€‰æ‹©æœ‰æ•ˆçš„Collection", "è¯·é€‰æ‹©æœ‰æ•ˆçš„Collection"]
            content = client.get_collection_content(collection_name)
            raw_content = client.get_raw_file_content(collection_name)
            client.logger.debug(f"Collection {collection_name} å†…å®¹åŠ è½½å®Œæˆ")
            return [content, raw_content]

        def show_confirm(filename):
            if not filename:
                client.logger.warning("æœªé€‰æ‹©æ–‡ä»¶å")
                return ["è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶", gr.update(visible=False), ""]
            collections = client.get_collections_by_filename(filename)
            confirm_msg = f"ç¡®å®šè¦åˆ é™¤æ–‡ä»¶ '{filename}' ç›¸å…³çš„ {len(collections)} ä¸ªcollectionså—ï¼Ÿ"
            client.logger.debug(f"æ˜¾ç¤ºåˆ é™¤ç¡®è®¤ï¼Œæ¶‰åŠ {len(collections)} ä¸ªCollection")
            return ["è¯·ç¡®è®¤æ˜¯å¦åˆ é™¤", gr.update(visible=True), confirm_msg]

        def hide_confirm():
            client.logger.info("å–æ¶ˆåˆ é™¤æ“ä½œ")
            return [gr.update(visible=False), "æ“ä½œå·²å–æ¶ˆ"]

        def delete_file(filename):
            client.logger.info(f"æ‰§è¡Œåˆ é™¤æ–‡ä»¶å {filename} çš„Collection")
            try:
                if not filename:
                    client.logger.warning("æœªé€‰æ‹©æ–‡ä»¶å")
                    return ["è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶", gr.update(visible=False), gr.update(), gr.update(), "", ""]
                status = client.delete_collections_by_filename(filename)
                remaining_files = client.list_filenames()
                remaining_collections = client.list_collections()
                client.logger.debug(f"åˆ é™¤æ“ä½œå®Œæˆï¼Œå‰©ä½™ {len(remaining_files)} ä¸ªæ–‡ä»¶")
                return [status, gr.update(visible=False), gr.update(choices=remaining_files, value=None), 
                        gr.update(choices=remaining_collections, value=None), "", ""]
            except Exception as e:
                client.logger.error(f"åˆ é™¤æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
                return [f"åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", gr.update(visible=False), gr.update(), gr.update(), "", ""]

        refresh_btn.click(fn=lambda: gr.update(choices=client.list_collections()), outputs=collection_dropdown)
        refresh_files_btn.click(fn=refresh_filenames_with_progress, outputs=[filename_dropdown, progress_display])
        collection_dropdown.change(fn=on_collection_select, inputs=[collection_dropdown], outputs=[info_display, content_display])
        filename_dropdown.change(fn=on_filename_select, inputs=[filename_dropdown], outputs=[collection_dropdown, info_display, content_display])
        delete_btn.click(fn=show_confirm, inputs=[filename_dropdown], outputs=[progress_display, confirm_dialog, confirm_text])
        confirm_btn.click(fn=delete_file, inputs=[filename_dropdown], outputs=[progress_display, confirm_dialog, filename_dropdown, collection_dropdown, info_display, content_display])
        cancel_btn.click(fn=hide_confirm, outputs=[confirm_dialog, progress_display])
        
    return app

if __name__ == "__main__":
    app = create_interface()
    app.queue()
    app.launch(server_name="0.0.0.0", share=False)